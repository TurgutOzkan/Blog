{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to Obtain Coefficients in Linear Regression?.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvTyCnUauFfGKKoDL5RvG4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurgutOzkan/DataSciencity/blob/master/How_to_Obtain_Coefficients_in_Linear_Regression%3F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBwH_hdvWfNH"
      },
      "source": [
        "In this post, we will go through the technical details of deriving parameters for linear regression. The post will directly dive into linear algebra and matrix representation of a linear model and show how to obtain coefficients without using the of-the-shelf Scikit-learn linear estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwiFz4waCGs2"
      },
      "source": [
        "Let's formulate our linear regression in the following :\n",
        "\n",
        "$  Y_i = \\beta_1 +  \\beta_2 X_{2i} + \\beta_3  X_{3i} + ...\\beta_k X_{ki} +  e_i \\tag{1} $\n",
        "\n",
        "where $\\beta$s are coefficients and $e$ is the error term.\n",
        "\n",
        "Using matrix notation, the above can be simplified as:\n",
        "\n",
        "$y = X \\beta + e  \\tag{2} $\n",
        "\n",
        "In order to better visualize and understand from linear algebraic perspective, let's express our equation in matrix form.\n",
        "\n",
        "$\\begin{bmatrix} Y_1\\\\Y_2 \\\\.\\\\. \\\\Y_n\\end{bmatrix} =\n",
        "\\begin{bmatrix}1 & X_{21} & X_{31}& .. & X_{k1}\\\\\n",
        "1 & X_{22} & X_{32} & .. &X_{k2}\\\\\n",
        ". & . & . & . & .\\\\\n",
        "1 & X_{2n} & X_{3n}& .. & X_{kn}\n",
        "\\end{bmatrix} \n",
        "\\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ .. \\\\ \\beta_k\n",
        "\\end{bmatrix} +\n",
        "\\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2 \\\\. \\\\ \\epsilon_n\\end{bmatrix}  \\tag{3}$  \n",
        "\n",
        "Using ordinary least squares (OLS), we will obtain $\\beta$s that will minimize the squared error. Below is simply $y - \\hat{y}$ squared, in other words, predicted values are subtracted from the observed values and then squared:\n",
        "\n",
        "$\n",
        "\\sum{\\epsilon_i}^2 = \\sum(Y_i - \\beta_1  - \\beta_2X_{2i}  \\space - \\space... \\space - \\beta_kX_{ki})^2 \\tag{4}\n",
        "$\n",
        "\n",
        "Expressing the squared errors ($\\sum{\\epsilon_i}^2)$ in matrix notation, we can express the same thing as $\\epsilon^T \\epsilon$ --transposing the error vector since:\n",
        "\n",
        " $\\epsilon^T \\epsilon = [\\epsilon_1 \\space \\epsilon_2 \\space.. \\space \\epsilon_n] \\begin{bmatrix} \\epsilon_1\\\\\\epsilon_2 \\\\.\\\\. \\\\\\epsilon_n\\end{bmatrix} =  \\epsilon_1^2 + \\epsilon_2^2 +.. \\space \\epsilon_n^2  = \\sum\\epsilon_i^2 \\tag{5}$\n",
        "\n",
        "From Equation (2):\n",
        "\n",
        "$ \\epsilon = y - X\\beta \\tag{6}$\n",
        "$\\epsilon^T \\epsilon = (y - X\\beta)^T(y- X\\beta) \\tag{7}$\n",
        "$=y^Ty - 2\\beta^TX^Ty + \\beta^TX^TX\\beta \\tag{8}$\n",
        "\n",
        "Note that we used distribution property of transpose when distributing the transpose sign in the paranthesis -- $(X\\beta)^T = \\beta^TX^T $. Also, note that $\\beta^TX^Ty$ is a scalar number so it is equal to its transpose $y^TX\\beta.$ That is how we got the $2\\beta^TX^Ty$ above.\n",
        "\n",
        "Now is the critical part. We will take partial derivates of Equation (8) with respect to $\\beta$. More formally:\n",
        "\n",
        "$\\frac{\\Large \\partial (\\epsilon^T \\epsilon)}{\\Large \\partial \\beta} = - 2X^Ty + 2X^TX\\beta \\tag{9}$\n",
        "\n",
        "and we set the above equation to zero because we want to obtain the global minimum where the slope is zero. Doing so:\n",
        "\n",
        "$(X^TX)\\beta = X^Ty \\tag{10}$, which can be expressed as:\n",
        "\n",
        "$\\beta = (X^TX)^{-1} \\space X^Ty \\tag{11} $\n",
        "\n",
        "Please note that Equation (11) yields the coefficients of our regression line if there is an inverse for $ (X^TX)$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQxvhtQnUNdz"
      },
      "source": [
        "#Implementation\n",
        "\n",
        "Now, let's test above equations within a code and compare it with scikit-learn results. For this test, we will use Scikitlearn Boston Housing data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCCjTnXHVHx0",
        "outputId": "35a0a294-c0fc-48ef-ddcf-4ea8216af82b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.linear_model import LinearRegression\n",
        "X, y = load_boston(return_X_y=True)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 13) (506,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-91Wxm8CeEE"
      },
      "source": [
        "# Define a functin for our own implementation.\n",
        "def get_regression_coefs(X, y):\n",
        "    \"\"\"Takes a feature matrix and adds 1s as intercept, and then\n",
        "    apply partial derivative on the squared errors\n",
        "    \"\"\"\n",
        "    X = np.c_[ np.ones(X.shape[0]), X]\n",
        "    XTX = ((X.T).dot(X))\n",
        "    XTX_inv = np.linalg.inv(XTX)\n",
        "    XTy = ((X.T).dot(y))\n",
        "    print(\"intercept and slopes:\", XTX_inv.dot(XTy) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4_Y0oFpU77P",
        "outputId": "2e0e631e-90f6-4492-af7d-5472b634b7cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "get_regression_coefs(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intercept and slopes: [ 3.64594884e+01 -1.08011358e-01  4.64204584e-02  2.05586264e-02\n",
            "  2.68673382e+00 -1.77666112e+01  3.80986521e+00  6.92224640e-04\n",
            " -1.47556685e+00  3.06049479e-01 -1.23345939e-02 -9.52747232e-01\n",
            "  9.31168327e-03 -5.24758378e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvkPicydVZyc"
      },
      "source": [
        "We obtained 36.45 as slope and other coefficients in order of they appear in our data. Now, let's see what scikitlearn will yield."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UGLBvsXVXqe",
        "outputId": "6fe13a6b-3526-4e55-85f0-c131bfdde96a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "reg = LinearRegression().fit(X, y)\n",
        "reg.score(X, y)\n",
        "print(\"slopes:\", reg.coef_)\n",
        "print(\"intercept:\", reg.intercept_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "slopes: [-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
            " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
            "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
            " -5.24758378e-01]\n",
            "intercept: 36.459488385090125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd4Ge89uV0Ot"
      },
      "source": [
        "Not surprisingly, we got the same result. In fact, Scikitlearn applies the same logic as Scipy the Ordinary Least Squares (scipy.linalg.lstsq). While one may not need to create every algorithm from scratch, for the aspiting data scientists, it is recommended to learn how to derive simple linear regression coefficients for one or more variables.\n",
        "\n",
        "Source: Basic Econometrics by Gujarati & Porter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr16u2-uGOwf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}